@Article{Reddy2018,
  author     = {Siva Reddy and Danqi Chen and Christopher D. Manning},
  journal    = {CoRR},
  title      = {CoQA: {A} Conversational Question Answering Challenge},
  year       = {2018},
  volume     = {abs/1808.07042},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1808-07042.bib},
  eprint     = {1808.07042},
  eprinttype = {arXiv},
  timestamp  = {Sun, 02 Sep 2018 15:01:56 +0200},
  url        = {http://arxiv.org/abs/1808.07042},
}

@Misc{Bhargava2021,
  author        = {Prajjwal Bhargava and Aleksandr Drozd and Anna Rogers},
  title         = {Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2110.01518},
  primaryclass  = {cs.CL},
}

@Article{Turc2019,
  author     = {Iulia Turc and Ming{-}Wei Chang and Kenton Lee and Kristina Toutanova},
  journal    = {CoRR},
  title      = {Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation},
  year       = {2019},
  volume     = {abs/1908.08962},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},
  eprint     = {1908.08962},
  eprinttype = {arXiv},
  timestamp  = {Thu, 29 Aug 2019 16:32:34 +0200},
  url        = {http://arxiv.org/abs/1908.08962},
}

@Article{Sanh2019,
  author  = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal = {ArXiv},
  title   = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  year    = {2019},
  volume  = {abs/1910.01108},
}

@InProceedings{Gardner2017,
  author = {Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord and Pradeep Dasigi and Nelson F. Liu and Matthew Peters and Michael Schmitz and Luke S. Zettlemoyer},
  title  = {AllenNLP: A Deep Semantic Natural Language Processing Platform},
  year   = {2017},
  eprint = {arXiv:1803.07640},
}

 
@TechReport{Rajpurkar2018,
  author     = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  title      = {Know {What} {You} {Don}'t {Know}: {Unanswerable} {Questions} for {SQuAD}},
  year       = {2018},
  month      = jun,
  note       = {arXiv:1806.03822 [cs] type: article},
  abstract   = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
  annote     = {Comment: ACL 2018},
  doi        = {10.48550/arXiv.1806.03822},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1806.03822.pdf:application/pdf},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Know {What} {You} {Don}'t {Know}},
  url        = {http://arxiv.org/abs/1806.03822},
  urldate    = {2022-12-11},
}

 
@Article{Rothe2020,
  author   = {Rothe, Sascha and Narayan, Shashi and Severyn, Aliaksei},
  journal  = {Transactions of the Association for Computational Linguistics},
  title    = {Leveraging {Pre}-trained {Checkpoints} for {Sequence} {Generation} {Tasks}},
  year     = {2020},
  issn     = {2307-387X},
  month    = dec,
  note     = {arXiv:1907.12461 [cs]},
  pages    = {264--280},
  volume   = {8},
  abstract = {Unsupervised pre-training of large neural models has recently revolutionized Natural Language Processing. By warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language Understanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT, GPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation, Text Summarization, Sentence Splitting, and Sentence Fusion.},
  annote   = {Comment: To be published in Transactions of the Association for Computational Linguistics (TACL)},
  doi      = {10.1162/tacl_a_00313},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1907.12461.pdf:application/pdf},
  keywords = {Computer Science - Computation and Language},
  url      = {http://arxiv.org/abs/1907.12461},
  urldate  = {2022-12-11},
}

@Comment{jabref-meta: databaseType:bibtex;}
